{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://shwetkm.github.io/upxlogo.png\"></img>\n",
    "\n",
    "# Python Advanced - Data Science Foundation\n",
    "\n",
    "**SET THE WORKING DIRECTORY SO THAT WE DO NOT HAVE TO THINK ABOUT PATH PROBLEMS**\n",
    "\n",
    "Please ensure all your folder names <font color='red'><b>do not have a space & use forward slash (/)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please ensure the path follows the rules we had discussed and ensure a '/' at the last\n",
    "your_local_path=\"C:/Users/tejks/Desktop/ML/practice/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_python(N):\n",
    "    d=0\n",
    "    for i in range(N):\n",
    "        d += (i % 3 - 1) * i\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 1.74 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit func_python(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6cd0f7bc3079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mspeed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "distance = [45,50,35]\n",
    "speed = [5,10,7]\n",
    "\n",
    "\n",
    "time = distance/speed \n",
    "print (time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to NumPy**\n",
    "\n",
    "NumPy (short form for Numerical Python) is the most fundamental package designed for scientific computing and data analysis. Most of the other packages such as pandas, statsmodels are built on top of it, and is an important package to know and learn about. At the heart of NumPy is a data structure called **ndarray**. ndarray is a basically a multi-dimensional array that is built specifically for the purpose of numerical data analysis. Python also has array capabilities, but they are more generic. <b>The advantage of using ndarray is that processing is extremely efficient and fast.</b> \n",
    "\n",
    "You can perform standard mathematical operations on either individual elements or complete array. The range of functions covered is <b>linear algebra, statistical operations, and other specialized mathematical operations</b>. For our purpose, we need to know about <b>ndarray and the range of mathematical functions that are relevant to our research purpose</b>. If you already know languages such as C, Fortran, then you can integrate NumPy code with code written in these languages and can pass NumPy arrays seamlessly. \n",
    "\n",
    "From an overall perspective, understanding of <b>NumPy will help us in using pandas effectively as it is built on top of NumPy and frequently we will also be using functions of NumPy in research work</b>. In the current session, we will only look at some of the most important features of NumPy. For a full listing of NumPy features, please visit http://wiki.scipy.org/Numpy_Example_List .\n",
    "\n",
    "Possible application of NumPy package in research work are:\n",
    "<i>\n",
    "+ Algorithmic operations such as sorting, grouping and set operations\n",
    "+ Performing repetitive operations on whole arrays of data without using loops\n",
    "+ Data merging and alignment operations\n",
    "+ Data indexing, filtering, and transformation on individual elements or whole arrays\n",
    "+ Data summarization and descriptive statistics\n",
    "</i>\n",
    "**Installing NumPy**\n",
    "\n",
    "In order to check if NumPy is installed, go to Package Manager and type NumPy. You will get a list of packages with names closely matching to NumPy. For our purpose, we need to focus on package named numpy 1.xx. If the package is not installed, click on Install. \n",
    "\n",
    "**Importing NumPy**\n",
    "\n",
    "In order to be able to use NumPy, first import it using import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above statement will import all of NumPy into your workspace. For starters its good, but if you are doing performance intensive work, then saving space is of importance. In such cases, you can import specific modules of NumPy by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 50 35]\n",
      "[ 5 10  7]\n",
      "[ 9.  5.  5.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How we have overcome the prior challenge using \n",
    "dist = array(distance)\n",
    "spd = np.array(speed)\n",
    "print (dist)\n",
    "print (spd)\n",
    "time= dist/spd\n",
    "print (time)\n",
    "len(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ndarray\n",
    "The most important data structure in NumPy is an n-dimensional array object. Using ndarray, you can store large multidimensional datasets in Python. Being an array, you can <b>perform mathematical operations on these arrays either one element at a time or on complete arrays without using loops</b>. The way to initialize an array object is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
      "[6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5]                                 # initialize a list and assign values to it\n",
    "b=[10,20,30,40,50]\n",
    "print (a + b)\n",
    "#print (a + 5)                               #Want to add '5' to all the elements in the list\n",
    "c = [val + 5 for val in a]           \n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 22 33 44 55]\n",
      "[ 6  7  8  9 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1000,   8000,  27000,  64000, 125000], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = array((1,2,3,4,5))                       # initializes an array a and assigns values to it\n",
    "b = array((10,20,30,40,50))                  # initializes another array b\n",
    "#print (a)\n",
    "#print (b)\n",
    "print(a+b) \n",
    "print (a+5) \n",
    "a**2\n",
    "b**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array:                 [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "\n",
      "anarray:               [ 5 10 15 20 25 30 35 40 45 50 55]\n",
      "\n",
      "\n",
      "onemorearray:          [ 3.4   3.46  3.52  3.58  3.64  3.7   3.76  3.82  3.88  3.94  4.  ]\n",
      "Rounded onemorearray:  [ 3.  3.  4.  4.  4.  4.  4.  4.  4.  4.  4.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.699999999999996"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = array(arange(10))                           #arange function here works as a sequence or counter in increments of 1\n",
    "print ('array:                ', c)\n",
    "print()\n",
    "print()\n",
    "\n",
    "anarray = array(np.arange(5,56,5))                 #arange function here works as a sequence or counter in increments of 5\n",
    "print ('anarray:              ', anarray)\n",
    "print()\n",
    "print()\n",
    "\n",
    "onemorearray = array(np.linspace(3.4,4,11.7))           #linspace function here creates 11 evenly spaced numbers over a specified interval\n",
    "print ('onemorearray:         ', onemorearray)\n",
    "print ('Rounded onemorearray: ', np.round(onemorearray,0))\n",
    "onemorearray.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each ndarray are associated two attributes: shape of the array, and type of the array. The <font color='red'><b>shape</b> of the array tells you about <b><u>dimensionality</u></b> of the array (rows and columns), and <b>type</b> of the array tells you about the <b><u>data type</u></b> contained</font> in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  32   45  123]\n",
      " [ 756   23 2123]\n",
      " [   1    2    3]\n",
      " [   4    5    6]\n",
      " [   6    5    4]\n",
      " [   3    2    1]\n",
      " [  78   89   87]\n",
      " [  76   54   31]]\n",
      "*****\n",
      "[[[[  32   45]\n",
      "   [ 123  756]]\n",
      "\n",
      "  [[  23 2123]\n",
      "   [   1    2]]\n",
      "\n",
      "  [[   3    4]\n",
      "   [   5    6]]]\n",
      "\n",
      "\n",
      " [[[   6    5]\n",
      "   [   4    3]]\n",
      "\n",
      "  [[   2    1]\n",
      "   [  78   89]]\n",
      "\n",
      "  [[  87   76]\n",
      "   [  54   31]]]]\n",
      "(2, 3, 2, 2)\n",
      "int32\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "data = np.array((32,45,123,756,23,2123,1,2,3,4,5,6,6,5,4,3,2,1,78,89,87,76,54,31))\n",
    "data = data.reshape(8,3)\n",
    "data2 = data.reshape(2,3,2,2)\n",
    "print(data)\n",
    "print (\"*****\")\n",
    "print (data2)\n",
    "print(data2.shape)\n",
    "print(data2.dtype)\n",
    "print(data2.size)\n",
    "#even_data = (data % 2 == 0)\n",
    "#data[even_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(50) + 5                        # Make a 50, 0 numbered single dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  6.,  6.,  6.,  6.],\n",
       "       [ 6.,  6.,  6.,  6.,  6.],\n",
       "       [ 6.,  6.,  6.,  6.,  6.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3,5)) + 6                     # Make a 15, 0 numbered 2 dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(30) + 7                         # Make a 30, 1 numbered single dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((5,9))                       # Make a 45, 1 numbered 2 dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(5)                            # creates a 5*5 identity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 3, 0, 0, 0, 0],\n",
       "       [0, 0, 5, 0, 0, 0],\n",
       "       [0, 0, 0, 3, 0, 0],\n",
       "       [0, 0, 0, 0, 4, 0],\n",
       "       [0, 0, 0, 0, 0, 5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(array([1,3,5,3,4,5]))        # Populate the disgonal elements only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas**\n",
    "\n",
    "Pandas is the primary package for performing data analysis tasks in Python. pandas derives its name from <b>PANel Data AnalysiS</b> and is the fundamental package that provides <b>relational data structures (think Excel, SQL type) and a host of capabilities to play with those data structures</b>. It is the most widely used package in Python for data analysis tasks, and is very good to work with <b>cross sectional, time series, and panel data analysis</b>. Python sits on top of NumPy and can be used with NumPy arrays and the functions in NumPy. How is pandas suited for a researcher’s needs:\n",
    "<i>\n",
    "+ Has a tabular data structure that can hold both <b>homogenous and heterogenous data</b>.\n",
    "+ Very <b>good indexing capabilities</b> that makes data alignment and merging easy.\n",
    "+ Good <b>time series functionality</b>. No need to use different data structures for time series and cross sectional data. Allows for both <b>ordered and unordered time-series data</b>.\n",
    "+ A host of <b>statistical functions</b> developed around NumPy and pandas that makes a researcher’s task easy and fast.\n",
    "+ Programming is lot <b>simpler and faster</b>.\n",
    "+ Easily handles <b>data manipulation and cleaning</b>.\n",
    "+ Easy to expand and shorten data sets. <b>Comprehensive merging, joins, and group by functionality to join multiple data sets</b>.\n",
    "</i>\n",
    "\n",
    "**Installing pandas** \n",
    "\n",
    "In order to check if pandas is installed, go to Package Manager and type pandas. By default, pandas already comes installed with a distribution of Canopy. If the package is not installed, click on Install.\n",
    "\n",
    "**Importing pandas**\n",
    "\n",
    "In order to be able to use NumPy, first import it using import statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                         # This will import pandas into your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                          # We will be using numpy functions so import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Structures in pandas**\n",
    "\n",
    "There are two basic data structures in pandas: <b><i>Series and DataFrame</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Series:** It is similar to a NumPy 1-dimensional array. In addition to the values that are specified by the programmer, <b>pandas attaches a label to each of the values</b>. If the labels are not provided by the programmer, then pandas assigns labels ( 0 for first element, 1 for second element and so on). A benefit of assigning labels to data values is that it becomes easier to perform manipulations on the dataset as the whole dataset becomes more of a dictionary where each value is associated with a label. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10\n",
       "1    20\n",
       "2    30\n",
       "3    40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series1 = pd.Series([10,20,30,40])\n",
    "series1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 20, 30, 40], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=4, step=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series1.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to specify custom index values rather than the default ones provided, you can do so using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one      10\n",
       "two      20\n",
       "three    30\n",
       "four     40\n",
       "five     50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2 = pd.Series([10,20,30,40,50], index=['one','two','three','four','five'])\n",
    "series2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ways of accesing elements in a Series object are similar to what we have seen in NumPy, and you can perform NumPy operations on Series data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2[2]                  # How to print 30 from the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2['three']            # Another way to print 30 from the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one      10\n",
       "three    30\n",
       "five     50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2[['one', 'three', 'five']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one     10\n",
       "two     20\n",
       "four    40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2[[0,1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one      14\n",
       "two      24\n",
       "three    34\n",
       "four     44\n",
       "five     54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2 + 4            # Add 4 to every element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one        1000\n",
       "two        8000\n",
       "three     27000\n",
       "four      64000\n",
       "five     125000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2 ** 3           # Cube every element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "four    40\n",
       "five    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series2[series2>30]    # Find all values greater than 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one      3.162278\n",
       "two      4.472136\n",
       "three    5.477226\n",
       "four     6.324555\n",
       "five     7.071068\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(series2)       # Get the square roots of all the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a dictionary, you can <b>create a Series data structure from that dictionary</b>. Suppose you are interested in EPS values for firms and the <i>values come from different sources and is not clean. In that case you dont have to worry about cleaning and aligning those values</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [90, 91, 92, 93, 94, 95]\n",
    "f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11, 96:13}\n",
    "firm1 = pd.Series(f1,index=years)                     # Extra element in VALUE not an issue\n",
    "firm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = {90:14,92:9, 93:13, 94:5}                      \n",
    "firm2 = pd.Series(f2,index=years)                   # Extra element in the KEY is handled as NaN\n",
    "firm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = {93:10, 94:12, 95: 13}\n",
    "firm3 = pd.Series(f3,index=years)\n",
    "firm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN stands for missing or NA values in pandas. Make use of isnull() function to find out if there are any missing values in the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(firm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key feature of Series data is structures is that you don't have to worry about data alignment. For example, if we have run a word count program on two different files and we have the following data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'finance': 10, 'earning': 5, 'debt':8}\n",
    "dict2 = {'finance' : 8, 'compensation':4, 'earning': 9}\n",
    "count1 = pd.Series(dict1)\n",
    "count2 = pd.Series(dict2)\n",
    "print (count1)\n",
    "print (count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to calculate the sum of common words in combined files, then we dont have to worry about data alignment. If we want to include all words, then we can take care of NaN values and compute the sum. By default, Series data structure ignores NaN values. NaN values stand for missing data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1+count2                                # Sum values on key based retrieval. Non matching keys become NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ix[2]                              # Print the row corresponding to the given Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ix[data.ticker=='DIS']            # Print the row corresponding to the ticker that is 'DIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Year'] = 2014                       # Add additional column Year. Populate value of 2014 in all the rows.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pricesquared'] = data.price**2      # Square all the prices and add another column 'pricesquared'.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['pricesquared']                  # Delete column 'pricesquared'\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pricesquared'] = np.NaN             # Create column 'pricesquared' with NaN values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sequence'] = np.arange(1,6)         # Add column 'sequence' and populate sequence of numbers from 1 to 5\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values                               # Get the values in a 2 dimensional form (without index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = data.drop(2)                    # Delete data at index 2\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [90, 91, 92, 93, 94, 95]\n",
    "print('-------------Firm 1------------')\n",
    "f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm1 = pd.Series(f1,index=years)\n",
    "print(firm1)\n",
    "print('-------------Firm 2------------')\n",
    "f2 = {90:14,92:9, 93:13, 94:5}\n",
    "firm2 = pd.Series(f2,index=years)\n",
    "print(firm2)\n",
    "print('-------------Firm 3------------')\n",
    "f3 = {93:10, 94:12, 95: 13}\n",
    "firm3 = pd.Series(f3,index=years)\n",
    "print(firm3)\n",
    "print('-------------Create empty dataframe------------')\n",
    "df1 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)    # Convert the 3 series to a DataFrame with 'years' as Index\n",
    "print(df1)\n",
    "print('-------------Populate the dataframe------------')\n",
    "df1.Firm1 = firm1\n",
    "df1.Firm2 = firm2\n",
    "df1.Firm3 = firm3\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df1.T         # Transpose the table\n",
    "print(dft)\n",
    "print('-----------------Delete Column = 90--------------------------')\n",
    "del dft[90]\n",
    "print(dft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a number of data structures to DataFrame such as a <b>ndarray, lists, dict, Series, and another DataFrame</b>. You can also <b>reindex</b> to confirm to data to a new index. Reindexing is a powerful feature that <b>allows you to access data in a number of different ways</b>, and also to confirm data to some new time series or other index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1 = df1.reindex([88,89,90,91,92,93,94,95,96,97,98])     # No data from 88-89 & from 96-98\n",
    "reindexdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1)                                                 # Year from 90 to 95                \n",
    "reindexdf1 = df1.reindex(np.arange(1988,2008))             # Make the years more meaningful. Then populate the values separately.\n",
    "reindexdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years1 = [90, 91, 92, 93, 94, 95]\n",
    "print('-------------Firm 4------------')\n",
    "f4 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm4 = pd.Series(f4,index=years)\n",
    "print(firm4)\n",
    "print('-------------Firm 5------------')\n",
    "f5 = {90:14,91:12, 92:9, 93:13, 94:5, 95:8}\n",
    "firm5 = pd.Series(f5,index=years)\n",
    "print(firm5)\n",
    "print('-------------Firm 6------------')\n",
    "f6 = {90:8, 91: 9, 92:9,93:10, 94:12, 95: 13}\n",
    "firm6 = pd.Series(f6,index=years)\n",
    "print(firm6)\n",
    "print('-------------Create  empty Data Frame------------')\n",
    "df2 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years1)\n",
    "print(df2)\n",
    "print('-------------Assign values to the Data Frame------------')\n",
    "df2.Firm1 = firm4\n",
    "df2.Firm2 = firm5\n",
    "df2.Firm3 = firm6\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf2 = df2.reindex([88,89,90,91,92,93,94,95,96,97,98], fill_value=0)             # Fill row values with 0\n",
    "reindexdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you have backfill (bfill) method to fill values backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf3 = df2.reindex([88,89,90,91,92,93,94,95,96,97,98], method='ffill') # Forwrd fill the last non-empty row to the left over\n",
    "reindexdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1+reindexdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexdf1.add(reindexdf3, fill_value=0)                      # Replace values for non-existing element to 0 so as help sum up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use NumPy functions inside DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])  # Create array of 3*3 dimensionality using standard normally distributed random vales\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(dataframe)                   # Absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x.max()-x.min()        # We have option to use a powerful lambda function feature. Get the range in a column\n",
    "dataframe.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.apply(f,axis=1)           # Apply lambda function on the row and not column as we did in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe)\n",
    "g = lambda x: x - np.mean(x)        \n",
    "dataframe.apply(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return pd.Series([np.mean(x), x.max(), x.min()], index=['mean','max','min'])\n",
    "print(dataframe)\n",
    "dataframe.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.sort_index(by='one'))    # Sort by column 'one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sort_index(by=['one','two'])   # Sort first by column 'one' then by column 'two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sum()                          # Column wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sum(axis=1)                    # Row wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe)\n",
    "dataframe.cumsum(axis=1)                 # Get cumulative sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have non-numeric data, then applying describe function would produce statistics such as <b>count, unique, frequency</b>. In addition to this, you can also calculate <b>skewness (skew), kurtosis (kurt), percent changes, difference, and other statistics</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Data**\n",
    "\n",
    "Pandas have a number of features to deal with <b>missing data</b>. We have seen an example of the case of descriptive statistics, where missing values are not taken into account while calculating the descriptive statistics. Missing data is denoted by NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [90, 91, 92, 93, 94, 95]\n",
    "f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}\n",
    "firm1 = pd.Series(f1,index=years)\n",
    "firm1\n",
    "f2 = {90:14,92:9, 93:13, 94:5}\n",
    "firm2 = pd.Series(f2,index=years)\n",
    "firm2\n",
    "f3 = {93:10, 94:12, 95: 13}\n",
    "firm3 = pd.Series(f3,index=years)\n",
    "firm3\n",
    "df3 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)\n",
    "df3\n",
    "df3.Firm1 = firm1\n",
    "df3.Firm2 = firm2\n",
    "df3.Firm3 = firm3\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadeleted = firm2.dropna()\n",
    "nadeleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of DataFrame, <b>if you use dropna, it deletes entire row by default</b>. Another way is to drop only <b>those rows that are all NA</b>. If you want to <b>drop columns, pass axis=1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf3 = df3.dropna()          # Drop row if any value in the row is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean2 = df3.dropna(how='all')   # Drop row if all values in the row are NaN\n",
    "clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columndrop = df3.dropna(axis=1)   # Drop column if any value in the column is NaN\n",
    "columndrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholddf = df3.dropna(thresh=2)    # Define threshold. In this case row having >= 2 NaN, drop the row\n",
    "thresholddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna1 = df3.fillna(0)      # Fill all NaN values with 0\n",
    "fillna1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna2 = df3.fillna({'Firm1':8, 'Firm2': 10, 'Firm3':14})   # We can specify what value should be replaced\n",
    "fillna2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna3 = df3.fillna(method='ffill')    # Forward fill NaNs for 1 level. Check Firm 2 -> 14 & 5\n",
    "fillna3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna4 = df3.fillna(method='bfill',limit=2)    # Back fill NaNs till 2 levels before. Check how Firm3 behaves\n",
    "fillna4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df3.mean())\n",
    "fillna5 = df3.fillna(df3.mean())  # We will substitute empty values with Mean of the Firm in concern\n",
    "fillna5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical Indexing**\n",
    "\n",
    "Hierarchical indexing allows you to have <b>index on an index (multiple index)</b>. It is an important feature of pandas using which you can <b>select subsets of data and perform independent analyses</b> on them. For example, suppose you have <b>firm prices data and the data is indexed by firm name</b>. On top of that, you can <b>index firms by industry</b>. Thus, industry becomes an index on top of firms. You can then perform analyses either on individual firm, or on group of firms in an industry, or on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data = pd.Series(np.random.randn(10),index=[['Ind1','Ind1','Ind1','Ind1','Ind2','Ind2','Ind2','Ind3','Ind3','Ind3'],\n",
    "                                              [1,2,3,4,1,2,3,1,2,3]])\n",
    "h_i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data['Ind3']   # Looks like Dictionary of a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data['Ind1':'Ind3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data[['Ind1','Ind3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data[:,3]   # 3rd sub index of main Index. We have 3 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data[:,4]   # 3rd sub index of main Index. We have 1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.unstack()        # Pivot a level of the index labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.unstack().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h_i_data)\n",
    "h_i_data.sum(level=1)  # Sum based on level 1, e.g. -0.407952 + 0.976813 + -0.360328 = 0.208534 (Between Industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data.sum(level=0)   # Sum based on Type of Industry (Within Industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel represents wide format panel data, stored as 3-dimensional array -> ndarray (items x major x minor)\n",
    "pdata = pd.Panel(dict((stk,web.get_data_yahoo(stk,start='6/1/2014',end='6/9/2014'))\n",
    "                      for stk in ['AAPL','GOOG','MSFT','DIS']))\n",
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata.ix[:,'6/1/2014':,'Low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pdata = pdata.to_frame()                     # A better way to present the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked_pdata\n",
    "stacked_pdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IO in pandas**\n",
    "\n",
    "In this section, we will focus on <b>I/O from text files, csv, excel, and sql files as well as getting data from web such as Yahoo! Finance</b>. Using functions in pandas, you can read data as a DataFrame object. \n",
    "\n",
    "**Reading a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedatacsv = pd.read_csv(your_local_path+'roedata.csv')\n",
    "#roedatacsv\n",
    "roedatacsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file does not have a header, then you can either let pandas assign default headers or you can specify custom headers. If you want industry name to be the index of DataFrame, you can achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedatacsv = pd.read_csv(your_local_path+'roedata.csv', index_col = 'Industry Name' )\n",
    "roedatacsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedatacsv = pd.read_csv(your_local_path+'roedata.csv', usecols = ['Industry Name','ROE'] ) # Prints on columns that we Use here\n",
    "roedatacsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_dem_data = pd.read_table(your_local_path+'capm_dem.dat', delimiter=' ',header = None)\n",
    "capm_dem_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustatdata = pd.read_csv(your_local_path+'compustat.csv', index_col = ['ggroup','gvkey'] )\n",
    "compustatdata = compustatdata.sort_index(0,ascending=False)\n",
    "compustatdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_data = pd.read_table(your_local_path+'crsp.output', sep='\\s+',header = None)\n",
    "crsp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Handling missing values**\n",
    "\n",
    "Some types of missing values are automatically identified by pandas as NaN while importing the data. Those types are NA, NULL, -1.#IND. Additionally, you can also specify a list of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roemissing = pd.read_csv(your_local_path+'roemissing.csv', na_values=['Null',-999] )\n",
    "roemissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roemissing = pd.read_csv(your_local_path+'roemissing.csv', na_values={'Number of firms':['NULL',-999],'ROE':['10000.00%']} )\n",
    "roemissing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedata = pd.read_csv(your_local_path+'roedata.csv')\n",
    "roedata.to_csv(your_local_path+'roedatawrite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roedata = pd.read_csv(your_local_path+'roedata.csv')\n",
    "roedata.to_csv(your_local_path+'roedatawrite2.csv', index=False, columns=['Industry Name','ROE']) # Index = 'False' & select your own columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Data**\n",
    "<p/>Resembles Joins in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame = pd.DataFrame({'key': range(5), \n",
    "                           'left_value': ['a', 'b', 'c', 'd', 'e']})\n",
    "right_frame = pd.DataFrame({'key': range(2, 7), \n",
    "                           'right_value': ['f', 'g', 'h', 'i', 'j']})\n",
    "print(left_frame)\n",
    "print('\\n')\n",
    "print(right_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left_frame, right_frame, on='key', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([left_frame, right_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([left_frame, right_frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
